{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699b53be-d436-4b23-9b69-46e6a0e26e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Data.Precipitation\", DoubleType(), True),\n",
    "    StructField(\"Date.Full\", DateType(), True),\n",
    "    StructField(\"Date.Month\", IntegerType(), True),\n",
    "    StructField(\"Date.Week of\", IntegerType(), True),\n",
    "    StructField(\"Date.Year\", IntegerType(), True),\n",
    "    StructField(\"Station.City\", StringType(), True),\n",
    "    StructField(\"Station.Code\", StringType(), True),\n",
    "    StructField(\"Station.Location\", StringType(), True),\n",
    "    StructField(\"Station.State\", StringType(), True),\n",
    "    StructField(\"Data.Temperature.Avg Temp\", IntegerType(), True),\n",
    "    StructField(\"Data.Temperature.Max Temp\", IntegerType(), True),\n",
    "    StructField(\"Data.Temperature.Min Temp\", IntegerType(), True),\n",
    "    StructField(\"Data.Wind.Direction\", IntegerType(), True),\n",
    "    StructField(\"Data.Wind.Speed\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"/FileStore/tables/weather.csv\", \n",
    "                    schema=schema, \n",
    "                    sep=\",\")\n",
    "# df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c105cb-21e2-434f-b64c-083480e662e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23227b48-c69a-4397-bbd2-a173f5289a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [\n",
    "    (\"James,,Smith\", [\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"], \"OH\", \"CA\"),\n",
    "    (\"Michael,,Rose,\", [\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"], \"NY\", \"NJ\"),\n",
    "    (\"Robert,,Williams\", [\"CSharp\", \"VB\"], [\"Spark\", \"Python\"], \"UT\", \"NV\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"languages_at_school\", ArrayType(StringType()), True),\n",
    "    StructField(\"languages_at_work\", ArrayType(StringType()), True),\n",
    "    StructField(\"current_state\", StringType(), True),\n",
    "    StructField(\"previous_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765a9148-fd16-40c8-b21d-cce0b48bad14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c8fe4c-52eb-480f-bad7-d0867d2b8426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(\"name\", explode(\"languages_at_school\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b2fd43-c32e-45bc-8dd9-1b15b9f35a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267a3b1b-a2ed-4d7a-97c0-2ef67aa08dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.select(\"name\", explode(\"languages_at_school\"), explode(\"languages_at_work\"))\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fb7b17-3a09-4119-8dfb-bdfa80dd7e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df3 = df.select(split(\"name\", \",,\").alias(\"firstMiddleAndLast\"))\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bfbb16-bc12-45e5-9712-21e61ea58ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df4 = df.filter(array_contains(df.languages_at_school, \"Java\")).select(\"name\", \"languages_at_school\")\n",
    "\n",
    "df4.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94a0c25-136b-47bb-9324-037a19d78bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "df5 = df.select(\"name\", array(\"current_state\", \"previous_state\").alias(\"state\"))\n",
    "\n",
    "df5.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c8e1d6-7262-4672-b9da-c8fcf03cfc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "dataDictionary = [\n",
    "    ('James', {'hair': 'black'}),\n",
    "    ('Michael', {'hair': 'brown'}),\n",
    "    ('Robert', {'hair': 'red'}),\n",
    "    ('Washington', {'hair': 'grey'}),\n",
    "    ('Jefferson', {'hair': 'brown'})\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"properties\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema=schema)\n",
    "\n",
    "df.display()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e377a5ad-aead-4665-b3af-0b0f62f79e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import map_keys, map_values\n",
    "\n",
    "df1 = df.select(\"name\", map_keys(\"properties\").alias(\"keys\"))\n",
    "df1.display()\n",
    "df2 = df.select(\"name\",  map_values(\"properties\").alias(\"values\"))\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fcce4e-f92b-49f9-8254-b54967e1f536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = df.select(explode(map_keys(\"properties\")))\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cb5b77-e72f-4f97-930f-dd62d344aece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(\"James\", 25), (\"Nitesh\", 30)])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "July_04_pactice_DEA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
