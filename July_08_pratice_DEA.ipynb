{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b88f29-4554-4e01-a1e2-f0adb5eb7fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7dc7847-153a-44d6-98e5-3ab0f734daca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").agg({\"salary\": \"sum\"}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09da1a1a-5bce-4752-8abe-24b30aca26df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").agg({\"salary\": \"min\"}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c3a90a-3155-46b0-a875-ddf6df259e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").agg({\"salary\": \"max\"}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45506069-f42a-4f51-a9fa-b28e90d19fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").agg({\"salary\": \"avg\"}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446d746f-a37a-4141-b530-25d0f52d4ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# groupby the department wit the sum of the salary and show all the employees name in the department, only employee name, department and salary, and sum of salary.\n",
    "# df.groupBy(\"department\").agg({\"salary\": \"sum\"}).join(df, \"department\").display()\n",
    "df.groupBy(\"department\").agg({\"salary\": \"sum\"}).join(df, \"department\").select(\"employee_name\", \"department\", \"salary\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a4b306-eda6-4781-b000-15adc3823a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "windowSpec = Window.partitionBy('department')\n",
    "ranked_df = df.withColumn('sum_slary', sum('salary').over(windowSpec))\n",
    "ranked_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c73022-dc5f-4b25-af3a-860b231710b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ranked_df.select(\"employee_name\", \"department\", \"salary\", \"sum_slary\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f794e9b-7b54-4098-a359-94299a21b370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, max\n",
    "\n",
    "ranked_df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9adc83a3-f67c-4cf8-a186-873cf3041879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "ranked_df.where(col(\"age\") > 30).display()\n",
    "ranked_df.filter(col(\"age\") > 30).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935be447-abed-4950-a90d-25af8218c3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('state')\n",
    "ranked_df2 = df.withColumn('sum_slary', sum('salary').over(windowSpec))\n",
    "ranked_df2.display()\n",
    "ranked_df2.filter(col(\"salary\") > 90000).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178f1446-fe7b-47dc-b730-a9e45a52e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/FileStore/tables/testdata_sunil.csv\"\n",
    "\n",
    "test_df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "test_df = test_df.withColumnRenamed(\"5.1\", \"sepal_length\").withColumnRenamed(\"3.5\", \"sepal_width\").withColumnRenamed(\"1.4\", \"petal_length\").withColumnRenamed(\"0.2\", \"petal_width\").withColumnRenamed(\"Iris-setosa\", \"class\")\n",
    "\n",
    "test_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cb9f2a-7b29-4aaf-9edc-c3f72aff7c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in test_df get the records with the highest petal length and sepal lenght\n",
    "a=test_df.agg(max(\"petal_length\").alias(\"max petal\")).collect() [0][0]\n",
    "b=test_df.agg(max(\"sepal_length\").alias(\"max sepal\")).collect()[0][0]\n",
    "print(\"max petal=\"+str(a))\n",
    "print(\"max sepal=\"+str(b))\n",
    "\n",
    "if a > b:\n",
    "  print(\"petal length is greater than sepal length\")\n",
    "else:\n",
    "  print(\"sepal length is greater than petal length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e50ede-68a0-41fd-8b0f-ce8c3f8e0fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.filter(col(\"petal_length\") == test_df.agg({'petal_length': 'max'}).collect()[0][0]).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d925a64-ab42-442a-b5f4-28e49fc12d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611435e0-6e7a-421c-852a-001abf6354b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74488234-e69a-40b6-8485-2f9e50a2007c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cast a age integer to string\n",
    "df.withColumn(\"age\", df.age.cast(\"string\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e701c7-aa46-4b90-971c-fb5f40034c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# multiply the salary by 10\n",
    "df.withColumn(\"salary\", df.salary * 10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6527dd-b21a-47c9-bb2b-7911cbd89d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "dataDF = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"M\", 3100),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", 4300),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"M\", 1400),\n",
    "    ((\"Maria\", \"Anne\", \"Jones\"), \"39192\", \"F\", 5500),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", -1)\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=dataDF, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bcd24f-042d-494e-81f8-a4b412aaaf86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumnRenamed(\"id\", \"new_id\").withColumnRenamed(\"salary\", \"new_salary\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d936d0d7-9b4a-4cfc-b6c3-2f66954c29a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename the nested name cloumn elements\n",
    "schema2 = StructType([\n",
    "    StructField(\"f1name\",StringType()),\n",
    "    StructField(\"m1iddlename\",StringType()),\n",
    "    StructField(\"l1name\",StringType())])\n",
    " \n",
    "df.select(col(\"name\").cast(schema2)).printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d9a88e1-aa1e-47b0-b983-d7da8e9d582c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    " \n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831463a4-e472-49a7-80fd-9a4f178a1527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat, lit\n",
    "# create a new column with employee name and department as a array in the new cloumn\n",
    "\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "df = df.withColumn(\"emp_dep\", array(df.employee_name, df.department))\n",
    "# df.withColumn(\"emp_dep\", concat(df.employee_name, lit(\"_\"), df.department)).display()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f595156b-0ce4-4d6f-b92f-b458ec51bc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show only disticnt records\n",
    "df.select(\"employee_name\", \"department\").distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4a0fd8-db3f-41e4-b499-0da3794f21b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicate record\n",
    "df.dropDuplicates([\"employee_name\", \"department\"]).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012208fb-f6a1-4396-be11-47545dfa7669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
    "\n",
    "temp_schema = StructType([StructField(\"address\", MapType(StringType(), StringType()))])\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"Sales\", 3000, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Michael\", \"Sales\", 4600, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Robert\", \"Sales\", 4100, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Maria\", \"Finance\", 3000, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"James\", \"Sales\", 3000, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}),\n",
    "    (\"Scott\", \"Finance\", 3300, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"Jen\", \"Finance\", 3900, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Jeff\", \"Marketing\", 3000, {\"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}),\n",
    "    (\"Kumar\", \"Marketing\", 2000, {\"address\": {\"city\": \"Seattle\", \"zip\": \"98101\"}}),\n",
    "    (\"Saif\", \"Sales\", 4100, {\"address\": {\"city\": \"New York\", \"zip\": \"10001\"}})\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('department', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True),\n",
    "    StructField('properties', temp_schema, True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e254914d-c825-4606-8359-339e56b80336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate minimum, maximum, average, and total salary per department.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, min, max, avg\n",
    "\n",
    "windowSpec = Window.partitionBy('department')\n",
    "df2 = df.withColumn('min_salary', min('salary').over(windowSpec)) \\\n",
    "    .withColumn('max_salary', max('salary').over(windowSpec)) \\\n",
    "    .withColumn('avg_salary', avg('salary').over(windowSpec)) \\\n",
    "    .withColumn('total_salary', sum('salary').over(windowSpec))\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27996c2d-8831-4fb6-83c7-3992025fb724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out employees who earn more than 3000 and are in the Sales department.\n",
    "df.filter((df.department == \"Sales\") & (df.salary > 3000)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bddc94c-1597-4ccb-b983-8eff0645d706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the maximum salary in the company using collect()[0][0].\n",
    "df.select(max(\"salary\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c4ff25-76f6-4a94-8f3c-786ce806c8df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If-Else Logic:\n",
    "# Create a new column called \"salary_level\":\n",
    "# \"Low\" if salary < 3000\n",
    "# \"Medium\" if salary between 3000â€“4000\n",
    "# \"High\" if salary > 4000\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df3 = df.withColumn(\"salary_level\", when(df.salary < 3000, \"Low\")\n",
    "    .when((df.salary >= 3000) & (df.salary <= 4000), \"Medium\")    \n",
    "    .otherwise(\"High\"))\n",
    "\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a356cd0-8e42-4c53-bfed-da491f470d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# withColumnRenamed:\n",
    "# Rename \"employee_name\" to \"name\".\n",
    "\n",
    "df4 = df.withColumnRenamed(\"name\", \"employee_name\")\n",
    "df4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadd483b-4a7a-4b95-ac64-1f41e8696524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update Nested Schema:\n",
    "# Update nested \"address.city\" to uppercase using withField.\n",
    "\n",
    "from pyspark.sql.functions import col, upper, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema for the properties column\n",
    "properties_schema = StructType([\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Parse the properties column as JSON\n",
    "df = df.withColumn(\"properties\", from_json(col(\"properties\"), properties_schema))\n",
    "\n",
    "# Update the nested \"address.city\" field to uppercase\n",
    "df5 = df.withColumn(\"properties\", \n",
    "    col(\"properties\").withField(\"address.city\", \n",
    "        upper(col(\"properties.address.city\"))))\n",
    "\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7268f3b2-7802-415f-abd4-ada474c7b599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows (exact matches).\n",
    "df.dropDuplicates().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c855ca-7f49-4451-a5ad-4632e392cdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign a dense rank of salary within each department using Window.partitionBy.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "windowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "ranked_df = df.withColumn('rank', dense_rank().over(windowSpec))\n",
    "ranked_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338ddf5f-56cc-4eeb-a451-6bf6b585b307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, struct\n",
    "\n",
    "df6 = df.withColumn(\n",
    "    \"properties\",\n",
    "    struct(\n",
    "        struct(\n",
    "            upper(col(\"properties.address.city\")).alias(\"City\"),\n",
    "            col(\"properties.address.zip\").alias(\"Zip\")\n",
    "        ).alias(\"Address\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df6)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "July_08_pratice_DEA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
